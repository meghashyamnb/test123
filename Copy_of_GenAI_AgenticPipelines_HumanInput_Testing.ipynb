{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghashyamnb/test123/blob/master/Copy_of_GenAI_AgenticPipelines_HumanInput_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Created by: [Bharath Kumar Hemachandran](mailto:bharathh@gmail.com) [Linkedin](https://www.linkedin.com/in/bharath-hemachandran/)\n",
        "\n",
        "\n",
        "\n",
        "**GenAI Agentic Workflows - Human Input Loop**\n",
        "=============================================="
      ],
      "metadata": {
        "id": "Y0Acv5ruJred"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "62pzguVf0CIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install crewai groq langchain_groq"
      ],
      "metadata": {
        "id": "OPqYULbuzyEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Set up Groq with Llama3\n",
        "os.environ[\"GROQ_API_KEY\"] = \"<your_groq_api_key>\"  # Replace with your API key\n",
        "model1 = \"groq/llama3-8b-8192\"\n",
        "model2 = \"llama-3.1-8b-instant\"\n",
        "llm = ChatGroq(model=model1)"
      ],
      "metadata": {
        "id": "JyzmFGCF0A1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Crew"
      ],
      "metadata": {
        "id": "vwUMrP0t0SSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxFkyhSCzkY6"
      },
      "outputs": [],
      "source": [
        "def create_and_run_crew(topic, feedback=None):\n",
        "    # Create your agents with specific roles\n",
        "    researcher = Agent(\n",
        "        role=\"Research Specialist\",\n",
        "        goal=\"Find and analyze data on the {topic} provided\",\n",
        "        backstory=\"You're an expert at finding relevant information and analyzing it thoroughly.\",\n",
        "        verbose=True,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    writer = Agent(\n",
        "        role=\"Content Writer\",\n",
        "        goal=\"Create high-quality content based on research of the {topic} provided\",\n",
        "        backstory=\"You transform complex research into clear, engaging content.\",\n",
        "        verbose=True,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    reviewer = Agent(\n",
        "        role=\"Quality Reviewer\",\n",
        "        goal=\"Ensure accuracy and quality of final output of the {topic}\",\n",
        "        backstory=\"You have a keen eye for detail and ensure all work meets high standards.\",\n",
        "        verbose=True,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    feedback_processor = Agent(\n",
        "        role=\"Feedback Analyzer\",\n",
        "        goal=\"Process human feedback and extract actionable insights\",\n",
        "        backstory=\"You specialize in understanding user needs and translating feedback into concrete improvements.\",\n",
        "        verbose=True,\n",
        "        llm=llm\n",
        "    )\n",
        "\n",
        "    # Define tasks for your agents\n",
        "    research_task = Task(\n",
        "        description=\"Research the given {topic} extensively and compile key findings\",\n",
        "        agent=researcher,\n",
        "        expected_output=\"Comprehensive research notes on the topic\"\n",
        "    )\n",
        "\n",
        "    # If there's feedback, include it in the writing task\n",
        "    writing_description = \"Using the research provided, create a well-structured article on the {topic} provided\"\n",
        "    if feedback:\n",
        "        writing_description += f\". Consider this feedback for improvement: {feedback}\"\n",
        "\n",
        "    writing_task = Task(\n",
        "        description=writing_description,\n",
        "        agent=writer,\n",
        "        expected_output=\"A complete draft article\",\n",
        "        context=[research_task]\n",
        "    )\n",
        "\n",
        "    review_task = Task(\n",
        "        description=\"Review the article for accuracy, clarity, and quality\",\n",
        "        agent=reviewer,\n",
        "        expected_output=\"Final polished article with review notes\",\n",
        "        context=[writing_task]\n",
        "    )\n",
        "\n",
        "    # Create a crew with sequential process\n",
        "    crew = Crew(\n",
        "        agents=[researcher, writer, reviewer, feedback_processor],\n",
        "        tasks=[research_task, writing_task, review_task],\n",
        "        verbose=True,\n",
        "        process=Process.sequential\n",
        "    )\n",
        "\n",
        "    # Execute the crew to complete all tasks\n",
        "    result = crew.kickoff(inputs={\"topic\": topic})\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle the feedback"
      ],
      "metadata": {
        "id": "p-BeWWp90m7h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_feedback(feedback, article):\n",
        "    \"\"\"Use the LLM to process feedback and determine if it's sufficient\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following feedback on an article:\n",
        "\n",
        "    FEEDBACK:\n",
        "    {feedback}\n",
        "\n",
        "    ARTICLE:\n",
        "    {article}\n",
        "\n",
        "    Task:\n",
        "    1. Determine if the feedback is detailed enough to guide improvements (YES/NO)\n",
        "    2. If NO, suggest what specific questions we should ask the user\n",
        "    3. If YES, summarize the key points for improvement in a structured format\n",
        "\n",
        "    Output your analysis in the following format:\n",
        "    SUFFICIENT: [YES/NO]\n",
        "    QUESTIONS: [if NO, list specific questions]\n",
        "    ANALYSIS: [if YES, structured improvement points]\n",
        "    \"\"\"\n",
        "    llm2 = ChatGroq(model=model2)\n",
        "    messages = [(\"system\",\"\"),(\"human\",prompt)]\n",
        "    response = llm2.invoke(messages).content\n",
        "    return response\n",
        "\n",
        "def collect_detailed_feedback(feedback_analysis, initial_feedback):\n",
        "    \"\"\"Collect detailed feedback from the user based on analysis\"\"\"\n",
        "    # Extract questions to ask the user\n",
        "    questions_start = feedback_analysis.find(\"QUESTIONS:\") + 10\n",
        "    questions_end = feedback_analysis.find(\"ANALYSIS:\") if \"ANALYSIS:\" in feedback_analysis else len(feedback_analysis)\n",
        "    questions = feedback_analysis[questions_start:questions_end].strip()\n",
        "\n",
        "    print(f\"\\nCould you provide more specific feedback? {questions}\")\n",
        "    additional_feedback = input(\"Your detailed feedback: \")\n",
        "\n",
        "    # Check if user wants to quit\n",
        "    if additional_feedback.lower() in ['quit', 'exit']:\n",
        "        return None, None\n",
        "\n",
        "    # Now process this additional feedback to see if it's sufficient\n",
        "    combined_feedback = f\"{initial_feedback} {additional_feedback}\"\n",
        "    return combined_feedback, additional_feedback\n"
      ],
      "metadata": {
        "id": "hSjTw4RJ0lIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the program\n",
        "\n"
      ],
      "metadata": {
        "id": "021vIyhj0xCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    topic = input(\"Enter the topic for the article: \")\n",
        "    # Check if user wants to quit at the beginning\n",
        "    if topic.lower() in ['quit', 'exit']:\n",
        "        print(\"Exiting the program.\")\n",
        "        return\n",
        "\n",
        "    iteration = 1\n",
        "    feedback = None\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\n=== Starting iteration {iteration} ===\")\n",
        "        if feedback:\n",
        "            print(f\"Incorporating feedback: {feedback}\")\n",
        "\n",
        "        # Run the crew and get the article\n",
        "        result = create_and_run_crew(topic, feedback)\n",
        "        print(\"\\n=== Final Article ===\")\n",
        "        print(result)\n",
        "\n",
        "        # Ask for human feedback\n",
        "        user_feedback = input(\"\\nHow would you rate this article? Please provide feedback (or type 'quit' or 'exit' to finish): \")\n",
        "\n",
        "        if user_feedback.lower() in ['quit', 'exit']:\n",
        "            print(\"Exiting the program.\")\n",
        "            break\n",
        "\n",
        "        # Process initial feedback\n",
        "        feedback_analysis = process_feedback(user_feedback, result)\n",
        "        print(\"\\nFeedback Analysis:\")\n",
        "        print(feedback_analysis)\n",
        "\n",
        "        # Check if initial feedback is sufficient\n",
        "        if \"SUFFICIENT: NO\" in feedback_analysis:\n",
        "            # Get more detailed feedback from user\n",
        "            combined_feedback, additional_feedback = collect_detailed_feedback(feedback_analysis, user_feedback)\n",
        "\n",
        "            # Check if user chose to quit during feedback collection\n",
        "            if combined_feedback is None:\n",
        "                print(\"Exiting the program.\")\n",
        "                break\n",
        "\n",
        "            # Process the combined feedback to see if it's now sufficient\n",
        "            second_analysis = process_feedback(combined_feedback, result)\n",
        "            print(\"\\nUpdated Feedback Analysis:\")\n",
        "            print(second_analysis)\n",
        "\n",
        "            # Check if the combined feedback is now sufficient\n",
        "            if \"SUFFICIENT: NO\" in second_analysis:\n",
        "                # If still insufficient, collect more feedback until sufficient\n",
        "                while \"SUFFICIENT: NO\" in second_analysis:\n",
        "                    print(\"\\nYour feedback is still not specific enough.\")\n",
        "                    combined_feedback, more_feedback = collect_detailed_feedback(second_analysis, combined_feedback)\n",
        "\n",
        "                    # Check if user chose to quit during additional feedback collection\n",
        "                    if combined_feedback is None:\n",
        "                        print(\"Exiting the program.\")\n",
        "                        return\n",
        "\n",
        "                    second_analysis = process_feedback(combined_feedback, result)\n",
        "                    print(\"\\nUpdated Feedback Analysis:\")\n",
        "                    print(second_analysis)\n",
        "\n",
        "                # When feedback becomes sufficient, extract the analysis\n",
        "                analysis_start = second_analysis.find(\"ANALYSIS:\") + 9\n",
        "                feedback = second_analysis[analysis_start:].strip()\n",
        "            else:\n",
        "                # The combined feedback is now sufficient\n",
        "                analysis_start = second_analysis.find(\"ANALYSIS:\") + 9\n",
        "                feedback = second_analysis[analysis_start:].strip()\n",
        "        else:\n",
        "            # Initial feedback was already sufficient\n",
        "            analysis_start = feedback_analysis.find(\"ANALYSIS:\") + 9\n",
        "            feedback = feedback_analysis[analysis_start:].strip()\n",
        "\n",
        "        iteration += 1\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TxqPuFwp06O1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the Pipeline"
      ],
      "metadata": {
        "id": "U2vp9aR84t2b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Test Objectives\n",
        "- Verify the core functionality of agent-based article generation.\n",
        "- Ensure accurate handling of human feedback.\n",
        "- Validate improvements in content quality through iterative feedback.\n",
        "- Assess error handling and robustness in various scenarios.\n",
        "- Evaluate system performance with different inputs."
      ],
      "metadata": {
        "id": "4PcbI9tG5ZL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Test Scope\n",
        "- **Functional Testing**: Ensuring expected outputs for given inputs.\n",
        "- **Edge Case Testing**: Handling unusual or extreme inputs.\n",
        "- **Error Handling Testing**: Ensuring stability in erroneous scenarios.\n",
        "- **Performance Testing**: Measuring response time and execution efficiency.\n",
        "- **Usability Testing**: Checking clarity and responsiveness of human feedback interactions."
      ],
      "metadata": {
        "id": "UnbT9z_J5hPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Test Strategy\n",
        "The testing will be conducted in three phases:\n",
        "\n",
        "- **Unit Testing**: Validate individual functions.\n",
        "- **Integration Testing**: Verify interactions between agents.\n",
        "- **End-to-End Testing**: Ensure a smooth workflow for article generation and feedback handling."
      ],
      "metadata": {
        "id": "bNsKitNZ5uJH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Test Scenarios and Steps\n",
        "\n",
        "### **4.1 Functional Tests**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-01: Generate Article** | Run `create_and_run_crew(\"Machine Learning\")` | Returns a valid article with research, writing, and review. |\n",
        "| **TC-02: Handle Feedback** | Provide structured feedback | Feedback analysis correctly determines sufficiency. |\n",
        "| **TC-03: Insufficient Feedback Handling** | Give vague feedback (e.g., \"Improve it\") | System asks for more details. |\n",
        "| **TC-04: Loop Iteration with Feedback** | Run multiple feedback cycles | Each cycle improves the article quality. |\n",
        "\n",
        "### **4.2 Edge Case Tests**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-05: Empty Topic Input** | Run `create_and_run_crew(\"\")` | Returns an error or prompts user for input. |\n",
        "| **TC-06: Non-Text Topic Input** | Input numbers/special characters as topic | Handles input gracefully or rejects invalid topics. |\n",
        "| **TC-07: Feedback Injection Attack** | Enter feedback with code/script | System sanitizes input and prevents execution. |\n",
        "| **TC-08: Extremely Long Feedback** | Provide a long feedback string (1000+ words) | System processes or truncates safely. |\n",
        "\n",
        "### **4.3 Error Handling Tests**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-09: API Key Missing** | Remove `GROQ_API_KEY` | System fails gracefully with an appropriate error. |\n",
        "| **TC-10: Invalid API Response** | Mock invalid response from LLM | System handles error and retries or exits safely. |\n",
        "| **TC-11: Unexpected User Input** | Provide gibberish input | System prompts user for clarification. |\n",
        "\n",
        "### **4.4 Performance Tests**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-12: Process Large Topic** | Use a long topic like \"History of AI from 1950 to 2025 with trends\" | System completes execution within a reasonable time. |\n",
        "| **TC-13: Multiple Concurrent Runs** | Run `create_and_run_crew()` in parallel | System handles concurrency without crashing. |\n",
        "\n"
      ],
      "metadata": {
        "id": "zYWGqXMA5_lh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Step-by-Step Testing Method\n",
        "\n",
        "### **5.1 Set Up Environment**\n",
        "```bash\n",
        "pip install crewai groq langchain_groq\n",
        "```\n",
        "Ensure API key is configured.\n",
        "\n",
        "### **5.2 Unit Testing**\n",
        "```python\n",
        "# Test process_feedback() with various inputs (detailed vs. vague)\n",
        "feedback_analysis = process_feedback(\"This needs improvement\", \"Sample article\")\n",
        "print(feedback_analysis)\n",
        "```\n",
        "```python\n",
        "# Test collect_detailed_feedback() by simulating user input\n",
        "feedback_analysis = process_feedback(\"Not clear enough\", \"Sample article\")\n",
        "print(collect_detailed_feedback(feedback_analysis, \"Not clear enough\"))\n",
        "```\n",
        "\n",
        "### **5.3 Integration Testing**\n",
        "```python\n",
        "# Run article generation and check outputs\n",
        "result = create_and_run_crew(\"Sample Topic\")\n",
        "print(result)\n",
        "```\n",
        "```python\n",
        "# Provide different types of feedback and observe iterations\n",
        "feedback = \"Needs better structure.\"\n",
        "feedback_analysis = process_feedback(feedback, result)\n",
        "print(feedback_analysis)\n",
        "```\n",
        "\n",
        "### **5.4 End-to-End Testing**\n",
        "```python\n",
        "# Run the full script and simulate user interactions\n",
        "main()\n",
        "```\n",
        "\n",
        "### **5.5 Performance & Stress Testing**\n",
        "```python\n",
        "# Increase topic complexity\n",
        "result = create_and_run_crew(\"History of AI from 1950 to 2025 with trends and key research milestones\")\n",
        "print(result)\n",
        "```\n",
        "```python\n",
        "# Run multiple instances in parallel\n",
        "import threading\n",
        "threads = []\n",
        "for i in range(5):\n",
        "    t = threading.Thread(target=create_and_run_crew, args=(\"Parallel Test\",))\n",
        "    threads.append(t)\n",
        "    t.start()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "### **5.5 Precision & Accuracy Evaluation**\n",
        "```python\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "def evaluate_output(expected_text, generated_texts):\n",
        "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # Load a sentence similarity model\n",
        "    similarities = [util.pytorch_cos_sim(model.encode(expected_text), model.encode(gen_text)) for gen_text in generated_texts]\n",
        "    threshold = 0.8  # Define a similarity threshold for \"correct\" outputs\n",
        "    relevant_outputs = sum(1 for sim in similarities if sim > threshold)\n",
        "    \n",
        "    precision = relevant_outputs / len(generated_texts) if generated_texts else 0\n",
        "    accuracy = sum(1 for sim in similarities if sim > threshold) / len(similarities)\n",
        "\n",
        "    return {\"precision\": precision, \"accuracy\": accuracy}\n",
        "\n",
        "# Example Usage\n",
        "expected = \"Machine Learning is a subset of AI that involves training models on data.\"\n",
        "generated = [\"ML is part of AI and uses training data.\", \"AI includes ML and deep learning.\"]\n",
        "metrics = evaluate_output(expected, generated)\n",
        "print(metrics)  # Expected: {'precision': some_value, 'accuracy': some_value}\n",
        "```\n",
        "\n",
        "### **5.6 Performance & Stress Testing**\n",
        "```python\n",
        "# Increase topic complexity\n",
        "result = create_and_run_crew(\"History of AI from 1950 to 2025 with trends and key research milestones\")\n",
        "print(result)\n",
        "```\n",
        "```python\n",
        "# Run multiple instances in parallel\n",
        "import threading\n",
        "threads = []\n",
        "for i in range(5):\n",
        "    t = threading.Thread(target=create_and_run_crew, args=(\"Parallel Test\",))\n",
        "    threads.append(t)\n",
        "    t.start()\n",
        "for t in threads:\n",
        "    t.join()\n",
        "```\n"
      ],
      "metadata": {
        "id": "57NwwdhN6_xn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fairness & Bias Testing for AI-Generated Content\n",
        "\n",
        "## **1. Objectives**\n",
        "- Identify and mitigate biases in AI-generated content.\n",
        "- Ensure fair representation across different demographic groups.\n",
        "- Evaluate sentiment neutrality and contextual appropriateness.\n",
        "- Apply AI fairness frameworks to measure and correct biases.\n",
        "\n",
        "## **2. Scope**\n",
        "- **Bias Detection:** Identify potential biases in text outputs.\n",
        "- **Demographic Fairness:** Ensure equal representation.\n",
        "- **Sentiment Analysis:** Detect and mitigate unintended bias in responses.\n",
        "- **Fairness Framework Integration:** Use AI fairness tools for evaluation.\n",
        "\n",
        "## **3. Test Scenarios**\n",
        "\n",
        "### **3.1 Bias Detection in Content**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-01: Gender Bias in Responses** | Generate responses to topics like careers, leadership, and intelligence. | No gender-stereotypical biases. |\n",
        "| **TC-02: Racial Bias in Responses** | Evaluate generated content for different cultural topics. | Neutral and inclusive outputs. |\n",
        "| **TC-03: Political Bias in Responses** | Generate text on political topics from various perspectives. | Balanced representation without partisanship. |\n",
        "\n",
        "### **3.2 Demographic Fairness in AI Output**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-04: Representation Across Demographics** | Generate content for different demographic groups and compare results. | Equal and fair representation. |\n",
        "| **TC-05: Bias in Text Summarization** | Summarize articles containing diverse viewpoints. | No preference for any particular group. |\n",
        "\n",
        "### **3.3 Sentiment Analysis for Bias Detection**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-06: Sentiment Polarity Check** | Analyze sentiment distribution in AI-generated text. | Neutral or contextually appropriate sentiment. |\n",
        "| **TC-07: Stereotype Detection** | Detect common stereotypes in AI-generated text. | AI avoids reinforcing stereotypes. |\n",
        "\n",
        "### **3.4 Fairness Framework Integration**\n",
        "| **Test Case** | **Description** | **Expected Outcome** |\n",
        "|--------------|----------------|----------------------|\n",
        "| **TC-08: AI Fairness 360 (AIF360) Evaluation** | Use `AIF360` to quantify bias in AI-generated text. | Fairness score above threshold. |\n",
        "| **TC-09: Fairlearn Bias Mitigation** | Apply `Fairlearn` to balance demographic representation. | Improved fairness in content distribution. |\n",
        "\n",
        "## **4. Implementation of Fairness Testing**\n",
        "\n",
        "### **4.1 Setup Environment**\n",
        "```bash\n",
        "pip install aif360 fairlearn scikit-learn pandas numpy\n",
        "```\n",
        "\n",
        "### **4.2 Bias Evaluation using AIF360**\n",
        "```python\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.datasets import StandardDataset\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_bias(texts):\n",
        "    df = pd.DataFrame({\"text\": texts})\n",
        "    dataset = StandardDataset(df, label_name=\"text\")\n",
        "    \n",
        "    reweighing = Reweighing()\n",
        "    transformed_dataset = reweighing.fit_transform(dataset)\n",
        "    \n",
        "    return transformed_dataset\n",
        "\n",
        "# Example Usage\n",
        "generated_texts = [\"Men are better at science.\", \"Women should focus on family.\"]\n",
        "print(evaluate_bias(generated_texts))\n",
        "```\n",
        "\n",
        "### **4.3 Bias Mitigation using Fairlearn**\n",
        "```python\n",
        "from fairlearn.reductions import DemographicParity\n",
        "\n",
        "def mitigate_bias(dataset):\n",
        "    demographic_parity = DemographicParity()\n",
        "    fairness_score = demographic_parity.fit(dataset)\n",
        "    return fairness_score\n",
        "\n",
        "# Example Usage\n",
        "data = evaluate_bias(generated_texts)\n",
        "print(mitigate_bias(data))\n",
        "```\n"
      ],
      "metadata": {
        "id": "LI8_tSTJ7uR-"
      }
    }
  ]
}